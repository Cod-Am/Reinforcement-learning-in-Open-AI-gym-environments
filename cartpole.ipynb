{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "# import matplotlib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from keras import Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def basic_policy(obs):\n",
    "    angle = obs[2]\n",
    "    return 0 if angle < 0 else 1\n",
    "totals = []\n",
    "for episode in range(500):\n",
    "    episode_rewards = 0\n",
    "    env.seed(episode)\n",
    "    obs = env.reset()\n",
    "    for step in range(200):\n",
    "        action = basic_policy(obs)\n",
    "        obs , reward , done , info = env.step(action)\n",
    "        episode_rewards += reward\n",
    "        if done:\n",
    "            break\n",
    "    totals.append(episode_rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.04408514, -0.03586387, -0.01099802, -0.00518236], dtype=float32)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obs = env.reset()\n",
    "obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42.47 9.303176876744846 24.0 71.0\n"
     ]
    }
   ],
   "source": [
    "print(np.mean(totals) , np.std(totals) , min(totals) , max(totals))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "    tf.keras.layers.Dense(4, activation='relu'),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_one_step(env, obs, loss_fn):\n",
    "    with tf.GradientTape() as tape:\n",
    "        left_proba = model(obs[np.newaxis])\n",
    "        action = (tf.random.uniform([1,1]) > left_proba)\n",
    "        y_target = tf.constant([[1.]]) - tf.cast(action, tf.float32)\n",
    "        loss = tf.reduce_mean(loss_fn(y_target , left_proba))\n",
    "    grads = tape.gradient(loss , model.trainable_variables)\n",
    "    # print('gradient:',grads)\n",
    "    obs , reward , done , info = env.step(int(action))\n",
    "    return obs, reward, done, info, grads\n",
    "\n",
    "def play_multiple_episodes(env , n_episodes , n_max_steps , loss_fn):\n",
    "    all_rewards = []\n",
    "    all_grads = []\n",
    "    for episode in range(n_episodes):\n",
    "        current_rewards = []\n",
    "        current_grads = []\n",
    "        obs = env.reset()\n",
    "        for step in range(n_max_steps):\n",
    "            obs, reward, done, info, grads = play_one_step(env, obs, loss_fn)\n",
    "            current_rewards.append(reward)\n",
    "            current_grads.append(grads)\n",
    "            if done:\n",
    "                break\n",
    "        all_rewards.append(current_rewards)\n",
    "        all_grads.append(current_grads)\n",
    "    return all_rewards,all_grads\n",
    "\n",
    "def discount_rewards(rewards, discount_factor):\n",
    "    discounted_rewards = np.zeros_like(rewards, dtype=np.float32)\n",
    "    cumulative = 0.0\n",
    "    for i in reversed(range(len(rewards))):\n",
    "        cumulative = rewards[i] + cumulative * discount_factor\n",
    "        discounted_rewards[i] = cumulative\n",
    "    return discounted_rewards\n",
    "\n",
    "def discount_and_normalize(all_rewards, discount_factor):\n",
    "    all_discounted_rewards = [discount_rewards(rewards, discount_factor) for rewards in all_rewards]\n",
    "    flat_rewards = np.concatenate(all_discounted_rewards)\n",
    "    rewards_mean = flat_rewards.mean()\n",
    "    rewards_std = flat_rewards.std()\n",
    "    return [(discounted_rewards-rewards_mean)/rewards_std for discounted_rewards in all_discounted_rewards]\n",
    "\n",
    "n_iterations = 150\n",
    "n_episodes_per_update = 10\n",
    "n_max_steps = 100\n",
    "discount_factor = 0.95\n",
    "\n",
    "optimizer = tf.keras.optimizers.Nadam(learning_rate = 0.01)\n",
    "loss_fn = tf.keras.losses.binary_crossentropy\n",
    "\n",
    "for iteration in range(n_iterations):\n",
    "    all_rewards , all_grads = play_multiple_episodes(env , n_episodes_per_update , n_max_steps , loss_fn)\n",
    "    all_final_rewards = discount_and_normalize(all_rewards , discount_factor)\n",
    "    all_mean_grad = []\n",
    "    for var_index in range(len(model.trainable_variables)):\n",
    "        mean_grads = tf.reduce_mean([final_reward * all_grads[episode_index][step][var_index]\n",
    "                                    for episode_index,final_reward in enumerate(all_final_rewards)\n",
    "                                    for  step, final_reward in enumerate(final_reward)],axis=0)\n",
    "        all_mean_grad.append(mean_grads)\n",
    "    optimizer.apply_gradients(zip(all_mean_grad, model.trainable_variables))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    }
   ],
   "source": [
    "model.save('models/cartpole_v1.h5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "reinforcement-learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
