{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gymnasium \n",
    "import matplotlib.pyplot as plt\n",
    "import ale_py\n",
    "import pygame\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras import Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 99\u001b[0m\n\u001b[0;32m     97\u001b[0m loss_fn \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mlosses\u001b[38;5;241m.\u001b[39mBinaryCrossentropy()\n\u001b[0;32m     98\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m iteration \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(iterations):\n\u001b[1;32m---> 99\u001b[0m     all_rewards, all_grads \u001b[38;5;241m=\u001b[39m \u001b[43mplay_multiple_episodes\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_episodes\u001b[49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_steps\u001b[49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    100\u001b[0m     all_final_rewards \u001b[38;5;241m=\u001b[39m discount_and_normalize(all_rewards , discount_factor)\n\u001b[0;32m    101\u001b[0m     all_mean_grads \u001b[38;5;241m=\u001b[39m []\n",
      "Cell \u001b[1;32mIn[7], line 67\u001b[0m, in \u001b[0;36mplay_multiple_episodes\u001b[1;34m(env, max_episodes, max_steps, loss_fn, model)\u001b[0m\n\u001b[0;32m     65\u001b[0m reward \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(max_steps):\n\u001b[1;32m---> 67\u001b[0m     obs , reward , done , truncated , info , grad \u001b[38;5;241m=\u001b[39m \u001b[43mplay_one_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreward\u001b[49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     68\u001b[0m     current_rewards\u001b[38;5;241m.\u001b[39mappend(reward)\n\u001b[0;32m     69\u001b[0m     current_grads\u001b[38;5;241m.\u001b[39mappend(grad)\n",
      "Cell \u001b[1;32mIn[7], line 47\u001b[0m, in \u001b[0;36mplay_one_step\u001b[1;34m(env, obs, reward, loss_fn, model)\u001b[0m\n\u001b[0;32m     44\u001b[0m grad \u001b[38;5;241m=\u001b[39m tape\u001b[38;5;241m.\u001b[39mgradient(loss, model\u001b[38;5;241m.\u001b[39mtrainable_variables)  \u001b[38;5;66;03m# Compute gradients\u001b[39;00m\n\u001b[0;32m     46\u001b[0m \u001b[38;5;66;03m# Take the action in the environment\u001b[39;00m\n\u001b[1;32m---> 47\u001b[0m obs, reward, done, truncated, info \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Correct the use of `action`\u001b[39;00m\n\u001b[0;32m     49\u001b[0m \u001b[38;5;66;03m# Debugging: Print reward and info\u001b[39;00m\n\u001b[0;32m     50\u001b[0m \u001b[38;5;66;03m# print(f\"Reward: {reward}, Done: {done}, Truncated: {truncated}\")\u001b[39;00m\n\u001b[0;32m     51\u001b[0m \n\u001b[0;32m     52\u001b[0m \u001b[38;5;66;03m# Render the environment (optional, for visualization)\u001b[39;00m\n\u001b[0;32m     53\u001b[0m env\u001b[38;5;241m.\u001b[39mrender()\n",
      "File \u001b[1;32md:\\IDEs\\anaconda\\envs\\reinforcement-learning\\lib\\site-packages\\gymnasium\\wrappers\\common.py:393\u001b[0m, in \u001b[0;36mOrderEnforcing.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m    391\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_has_reset:\n\u001b[0;32m    392\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ResetNeeded(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot call env.step() before calling env.reset()\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 393\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\IDEs\\anaconda\\envs\\reinforcement-learning\\lib\\site-packages\\gymnasium\\core.py:322\u001b[0m, in \u001b[0;36mWrapper.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m    318\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep\u001b[39m(\n\u001b[0;32m    319\u001b[0m     \u001b[38;5;28mself\u001b[39m, action: WrapperActType\n\u001b[0;32m    320\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mtuple\u001b[39m[WrapperObsType, SupportsFloat, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any]]:\n\u001b[0;32m    321\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Uses the :meth:`step` of the :attr:`env` that can be overwritten to change the returned data.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 322\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\IDEs\\anaconda\\envs\\reinforcement-learning\\lib\\site-packages\\gymnasium\\wrappers\\common.py:285\u001b[0m, in \u001b[0;36mPassiveEnvChecker.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m    283\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m env_step_passive_checker(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv, action)\n\u001b[0;32m    284\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 285\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\IDEs\\anaconda\\envs\\reinforcement-learning\\lib\\site-packages\\ale_py\\env.py:250\u001b[0m, in \u001b[0;36mAtariEnv.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m    248\u001b[0m reward \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[0;32m    249\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(frameskip):\n\u001b[1;32m--> 250\u001b[0m     reward \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39male\u001b[38;5;241m.\u001b[39mact(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_action_set[action])\n\u001b[0;32m    251\u001b[0m is_terminal \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39male\u001b[38;5;241m.\u001b[39mgame_over(with_truncation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m    252\u001b[0m is_truncated \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39male\u001b[38;5;241m.\u001b[39mgame_truncated()\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "gymnasium.register_envs(ale_py)\n",
    "env = gymnasium.make(\"ALE/MsPacman-v5\" , render_mode = \"human\")\n",
    "\n",
    "model = Sequential([\n",
    "    tf.keras.layers.InputLayer(input_shape=(210,160,3)),\n",
    "\n",
    "    tf.keras.layers.Conv2D(filters = 32 , kernel_size=(8,8) , strides = 4 , activation = \"relu\"),\n",
    "    tf.keras.layers.Conv2D(filters = 64 , kernel_size=(4,4) , strides = 4 , activation = \"relu\"),\n",
    "    tf.keras.layers.Conv2D(filters = 64 , kernel_size=(3,3) , strides = 4 , activation = \"relu\"),\n",
    "\n",
    "    tf.keras.layers.MaxPooling2D(pool_size=(2,2)),\n",
    "\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(units=512, activation='relu'),\n",
    "    tf.keras.layers.Dense(units=256, activation='relu'),\n",
    "\n",
    "    # Output layer: 1 unit per possible action with sigmoid activation\n",
    "    tf.keras.layers.Dense(units=8, activation='sigmoid')\n",
    "])\n",
    "\n",
    "def play_one_step(env, obs, reward, loss_fn, model):\n",
    "    normalized_obs = obs / 255.0  # Normalize the observation\n",
    "    with tf.GradientTape() as tape:\n",
    "        yprob = model(normalized_obs[np.newaxis])  # Forward pass to get probabilities\n",
    "        yprob = yprob.numpy().flatten()  # Flatten the array to simplify the index search\n",
    "        \n",
    "        # Find indices of max and min probabilities\n",
    "        max_index = np.argmax(yprob)  # Index of max probability\n",
    "        min_index = np.argmin(yprob)  # Index of min probability\n",
    "        if min_index == 0:\n",
    "            temp_arr = np.delete(yprob, 0)\n",
    "            min_index = np.argmin(temp_arr) + 1\n",
    "\n",
    "        # Choose the action based on random comparison\n",
    "        action_bool = np.max(yprob) > np.random.uniform(0, 1)\n",
    "        action = max_index if action_bool else min_index  # Select action\n",
    "\n",
    "        ytarget = np.zeros_like(yprob, dtype=np.float32)  # One-hot target vector\n",
    "        ytarget[action] = 1  # Set target at the action index\n",
    "\n",
    "        # Compute the loss using the binary cross-entropy loss function\n",
    "        loss = tf.reduce_mean(loss_fn(yprob, ytarget))\n",
    "    \n",
    "    grad = tape.gradient(loss, model.trainable_variables)  # Compute gradients\n",
    "\n",
    "    # Take the action in the environment\n",
    "    obs, reward, done, truncated, info = env.step(action)  # Correct the use of `action`\n",
    "\n",
    "    # Debugging: Print reward and info\n",
    "    # print(f\"Reward: {reward}, Done: {done}, Truncated: {truncated}\")\n",
    "\n",
    "    # Render the environment (optional, for visualization)\n",
    "    env.render()\n",
    "\n",
    "    return obs, reward, done, truncated, info, grad\n",
    "\n",
    "\n",
    "def play_multiple_episodes(env , max_episodes , max_steps , loss_fn , model):\n",
    "    all_rewards = []\n",
    "    all_grads = []\n",
    "    for episode in range(max_episodes):\n",
    "        current_rewards = []\n",
    "        current_grads = []\n",
    "        obs , info = env.reset()\n",
    "        reward = 0\n",
    "        for step in range(max_steps):\n",
    "            obs , reward , done , truncated , info , grad = play_one_step(env , obs, reward , loss_fn , model)\n",
    "            current_rewards.append(reward)\n",
    "            current_grads.append(grad)\n",
    "            if done or truncated:\n",
    "                break\n",
    "        all_rewards.append(current_rewards)\n",
    "        all_grads.append(current_grads)\n",
    "    return all_rewards,all_grads\n",
    "\n",
    "def discount_rewards(rewards , discount_factor):\n",
    "    discounted_rewards = np.zeros_like(rewards , np.float32)\n",
    "    cummulative = 0.0\n",
    "    for index in reversed(range(len(rewards))):\n",
    "        cummulative = rewards[index] + cummulative * discount_factor\n",
    "        discounted_rewards[index] = cummulative\n",
    "    return discounted_rewards\n",
    "\n",
    "def discount_and_normalize(all_rewards, discount_factor):\n",
    "    all_discount_rewards = [discount_rewards(rewards , discount_factor) for rewards in all_rewards]\n",
    "    flat_rewards = np.concatenate(all_discount_rewards)\n",
    "    mean_rewards = flat_rewards.mean()\n",
    "    std_rewards = flat_rewards.std()\n",
    "    return [((discounted_rewards - mean_rewards)/std_rewards) for discounted_rewards in all_discount_rewards]\n",
    "\n",
    "iterations = 20\n",
    "max_episodes = 1000\n",
    "max_steps = 1000\n",
    "discount_factor = 0.95\n",
    "\n",
    "optimizer = tf.optimizers.Nadam(learning_rate = 0.01)\n",
    "loss_fn = tf.losses.BinaryCrossentropy()\n",
    "for iteration in range(iterations):\n",
    "    all_rewards, all_grads = play_multiple_episodes(env , max_episodes , max_steps , loss_fn,model)\n",
    "    all_final_rewards = discount_and_normalize(all_rewards , discount_factor)\n",
    "    all_mean_grads = []\n",
    "    for var_index in range(len(model.trainable_variables)):\n",
    "        mean_grads = tf.reduce_mean(\n",
    "            [final_reward * all_grads[episode_index][step][var_index]\n",
    "             for episode_index , final_rewards in enumerate(all_final_rewards)\n",
    "             for step , final_reward in enumerate(final_rewards)] , axis = 0)\n",
    "        all_mean_grads.append(mean_grads)\n",
    "    optimizer.apply_gradients(all_mean_grads , model.trainable_variables)\n",
    "\n",
    "pygame.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "pygame.quit()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "reinforcement-learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
