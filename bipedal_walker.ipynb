{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "# import pandas as pd\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras import Sequential\n",
    "import gymnasium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"BipedalWalker-v3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "model = Sequential([\n",
    "    tf.keras.layers.Dense(32, activation='relu'),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Dense(500, activation='relu'),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Dense(500, activation='relu'),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Dense(4, activation='tanh')  # Output actions\n",
    "])\n",
    "\n",
    "# State consists of hull angle speed, angular velocity, horizontal speed, vertical speed, position of joints and \n",
    "# joints angular speed, legs contact with ground, and 10 lidar rangefinder measurements. There are no coordinates in the state vector.\n",
    "# obs[0]  : hull angle speed\n",
    "# obs[1] : hull angular velocity\n",
    "# obs[2] : horizontal speed\n",
    "# obs[3] : vertical speed\n",
    "# obs[4 and 6] : position of joints\n",
    "# obs[5 and 7] : joints angular speed\n",
    "# obs[8 and 9] : legs contact with ground\n",
    "# obs[10 to 19] : 10 lidar rangefinder measurements\n",
    "\n",
    "\n",
    "def normalize_obs(obs):\n",
    "    obs_min = np.array([3.14, 5., 5., 5., 3.14, 5., 3.14, 5., 5., 3.14, 5., 3.14, 5., 5., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])\n",
    "    obs_max = np.array([-3.14, -5., -5., -5., -3.14, -5., -3.14, -5., -0., -3.14, -5., -3.14, -5., -0., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.])\n",
    "    return (obs - obs_min) / (obs_max - obs_min) * 2 - 1\n",
    "\n",
    "# (abs(obs[4] - obs[5]) * gamma ** 3)\n",
    "def play_one_step(env, obs, loss_fun , gamma):\n",
    "    with tf.GradientTape() as tape:\n",
    "        obs = normalize_obs(obs)\n",
    "        forward_velocity = obs[2] * gamma ** -3\n",
    "        stability_penalty = -abs(obs[0]) - abs(obs[1])  # Penalize hull instability\n",
    "        leg_swing_reward = -abs(obs[4] - obs[6]) * gamma + -abs(obs[5] - obs[7]) * gamma ** 2\n",
    "        swing_speed_reward = +abs(obs[8] - obs[10]) * gamma + -abs(obs[9] - obs[11]) * gamma ** 2\n",
    "        contact_reward = (-obs[12] + obs[13]) * gamma ** -2  # Encourage legs to make ground contact\n",
    "        heuristic_function = forward_velocity + stability_penalty + leg_swing_reward + contact_reward\n",
    "        modified_obs = np.append(obs, heuristic_function)\n",
    "        y_prob = model(modified_obs[np.newaxis])\n",
    "\n",
    "        action = []\n",
    "        y_target = []\n",
    "\n",
    "        for state in y_prob[0]:\n",
    "            np.random.seed(np.random.randint(low = 0 , high = np.iinfo(np.int32).max))\n",
    "            state_bool = (np.random.uniform(low = -1 , high = 1 , size=[1]) > state)\n",
    "            int_state = 1 if state_bool else -1\n",
    "            action.append(int_state)\n",
    "            target = -int_state\n",
    "            y_target.append(target)\n",
    "\n",
    "        y_target = tf.convert_to_tensor(y_target, dtype=tf.float32)\n",
    "        y_prob = tf.squeeze(y_prob)  # Remove extra dimensions if necessary\n",
    "\n",
    "        # Calculating loss\n",
    "        loss = tf.reduce_mean(loss_fun(y_target, y_prob))\n",
    "\n",
    "    grads = tape.gradient(loss, model.trainable_variables)\n",
    "\n",
    "    obs, reward, done, info = env.step(action)\n",
    "    env.render()\n",
    "\n",
    "    return obs, reward, done, info, grads\n",
    "\n",
    "def play_multiple_episodes(env , max_episodes , max_steps , loss_fn):\n",
    "    gamma = 0.9\n",
    "    all_rewards = []\n",
    "    all_grads = []\n",
    "    for episode in range(max_episodes):\n",
    "        current_rewards = []\n",
    "        current_grads = []\n",
    "        env.seed(episode)    \n",
    "        obs = env.reset()\n",
    "        for step in range(max_steps):\n",
    "            obs, reward, done, info, grads = play_one_step(env, obs , loss_fn , gamma)\n",
    "            current_rewards.append(reward)\n",
    "            current_grads.append(grads)\n",
    "            if done:\n",
    "                break\n",
    "        all_rewards.append(current_rewards)\n",
    "        all_grads.append(current_grads)\n",
    "    return all_rewards, all_grads\n",
    "\n",
    "def discount_rewards(rewards, discount_factor):\n",
    "    discounted_reward = np.zeros_like(rewards, dtype=np.float32)\n",
    "    cummulated = 0.0\n",
    "    for index in reversed(range(len(rewards))):\n",
    "        cummulated = rewards[index] + cummulated * discount_factor\n",
    "        discounted_reward[index] = cummulated \n",
    "    return discounted_reward\n",
    "def discount_and_normalize(all_rewards, discount_factor):\n",
    "    all_discounted_rewards = [discount_rewards(reward, discount_factor) for reward in all_rewards]\n",
    "    flat_rewards = np.concatenate(all_discounted_rewards)\n",
    "    rewards_mean = flat_rewards.mean()\n",
    "    rewards_std = flat_rewards.std()\n",
    "    return [(discounted_reward - rewards_mean)/rewards_std for discounted_reward in all_discounted_rewards]\n",
    "\n",
    "n_iterations = 1600\n",
    "n_episodes_per_update = 10\n",
    "n_max_steps = 500\n",
    "discount_factor = 0.95\n",
    "\n",
    "optimizer = tf.keras.optimizers.Nadam(learning_rate = 0.01)\n",
    "loss_fn = tf.keras.losses.binary_crossentropy\n",
    "\n",
    "for iteration in range(n_iterations):\n",
    "    print(iteration)\n",
    "    all_rewards , all_grads = play_multiple_episodes(env , n_episodes_per_update , n_max_steps , loss_fn)\n",
    "    all_final_rewards = discount_and_normalize(all_rewards , discount_factor)\n",
    "    all_mean_grad = []\n",
    "    for var_index in range(len(model.trainable_variables)):\n",
    "        mean_grads = tf.reduce_mean([final_reward * all_grads[episode_index][step][var_index]\n",
    "                                    for episode_index,final_reward in enumerate(all_final_rewards)\n",
    "                                    for  step, final_reward in enumerate(final_reward)],axis=0)\n",
    "        all_mean_grad.append(mean_grads)\n",
    "    optimizer.apply_gradients(zip(all_mean_grad, model.trainable_variables))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENV=gymnasium.make('Humanoid-v5',render_mode = 'human')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENV.reset()\n",
    "ENV.render()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "reinforcement-learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
